import requests
from bs4 import BeautifulSoup
import pandas as pd
from urllib.parse import urljoin
from collections import defaultdict
from datetime import datetime
from pathlib import Path
import urllib3
import arabic_reshaper
from bidi.algorithm import get_display
from openpyxl import load_workbook
from openpyxl.styles import PatternFill
from openpyxl.utils.dataframe import dataframe_to_rows

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

BASE_URL = "https://www.hackeru.co.il"
BLOG_HOME = urljoin(BASE_URL, "/blog")
CATEGORY_PREFIX = "/blog/category/"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

def get_soup(url):
    response = requests.get(url, headers=headers, verify=False)
    response.raise_for_status()
    return BeautifulSoup(response.text, 'html.parser')

def extract_categories(soup):
    categories = []
    for link in soup.find_all("a", href=True):
        href = link['href']
        if href.startswith(CATEGORY_PREFIX):
            full_url = urljoin(BASE_URL, href)
            reshaped = arabic_reshaper.reshape(link.text.strip())
            display_name = get_display(reshaped)
            categories.append((display_name, full_url))
    return list(set(categories))

def is_internal_link(href):
    return href.startswith("/") or href.startswith(BASE_URL)

def count_internal_links(article_url):
    try:
        soup = get_soup(article_url)
        main_text_sections = soup.find_all("div", class_="main-text")
        internal_links = []

        for section in main_text_sections:
            anchors = section.find_all("a", href=True)
            for anchor in anchors:
                href = anchor['href']
                if href.startswith("/") or BASE_URL in href:
                    internal_links.append(href)

        return len(set(internal_links))
    except Exception as e:
        print(f"‚ö†Ô∏è Error in {article_url}: {e}")
        return 0

def extract_articles(category_name, category_url):
    soup = get_soup(category_url)
    posts = []
    post_cards = soup.select("a.post-card")
    for card in post_cards:
        title_element = card.select_one(".post-card__title h3")
        if title_element:
            post_title = title_element.get_text(strip=True)
            post_url = urljoin(BASE_URL, card['href'])
            internal_links = count_internal_links(post_url)
            posts.append({
                "Category": category_name,
                "Title": post_title,
                "URL": post_url,
                "# of Internal Links": internal_links
            })
    return posts

def generate_color(index):
    palette = ["FFCCCC", "CCFFCC", "CCCCFF", "FFF2CC", "F4CCCC", "D9EAD3", "D0E0E3", "FCE5CD", "EAD1DC"]
    return palette[index % len(palette)]

def export_to_excel(df, summary_df, filename):
    output_path = Path.cwd() / filename
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name="All Posts", index=False)
        summary_df.to_excel(writer, sheet_name="Summary", index=False)

    # Re-open workbook to style cells
    wb = load_workbook(output_path)
    ws = wb["All Posts"]

    category_colors = {}
    for idx, row in enumerate(ws.iter_rows(min_row=2, max_row=ws.max_row), start=2):
        category = row[0].value
        if category not in category_colors:
            category_colors[category] = generate_color(len(category_colors))
        fill = PatternFill(start_color=category_colors[category], end_color=category_colors[category], fill_type="solid")
        for cell in row:
            cell.fill = fill

    wb.save(output_path)
    return output_path

def main():
    print("üîç Fetching blog homepage...")
    soup = get_soup(BLOG_HOME)

    print("üìÇ Extracting blog categories...")
    categories = extract_categories(soup)

    all_data = []
    for name, url in categories:
        print(f"üìë Scraping category: {name} ({url})")
        posts = extract_articles(name, url)
        if posts:
            all_data.extend(posts)
        else:
            print(f"‚ö†Ô∏è No blog posts found for category: {name}")

    if not all_data:
        print("‚ö†Ô∏è No data to export.")
        return None

    df = pd.DataFrame(all_data)

    # Grouped summary
    summary_df = df.groupby("Category").agg({
        "Title": "count",
        "# of Internal Links": "sum"
    }).rename(columns={"Title": "# of Articles"}).reset_index()

    timestamp = datetime.now().strftime("%Y-%m-%d")
    filename = f"blog_analysis_{timestamp}.xlsx"
    output_path = export_to_excel(df, summary_df, filename)

    print(f"\n‚úÖ Done! Exported to {output_path}")
    return output_path

if __name__ == "__main__":
    main()
